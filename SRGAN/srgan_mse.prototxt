name: "SRGAN_MSE"
#-----------   generative  ------------
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    hdf5_data_param {
        source: "examples/SRGAN/train_75s.txt"
        batch_size: 8
    }
    include: {
                phase: TRAIN
             }
}
layer {
    name: "data"
    type: "HDF5Data"
    top: "data"
    top: "label"
    hdf5_data_param {
        source: "examples/SRGAN/test_75s.txt"
        batch_size: 2
    }
    include: {
                phase: TEST
             }

}
layer {
    name: "conv_g1"
    type: "Convolution"
    bottom: "data"
    top: "conv_g1"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "relu_g1"
    type: "ReLU"
    bottom: "conv_g1"
    top: "conv_g1"
}
layer {
    name: "conv_g2"
    type: "Convolution"
    bottom: "conv_g1"
    top: "conv_g2"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g2"
    type: "BatchNorm"
    bottom: "conv_g2"
    top: "conv_g2"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g2"
    type: "ReLU"
    bottom: "conv_g2"
    top: "conv_g2"
}
layer {
    name: "conv_g3"
    type: "Convolution"
    bottom: "conv_g2"
    top: "conv_g3"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g3"
    type: "BatchNorm"
    bottom: "conv_g3"
    top: "conv_g3"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum1"
    type: "Eltwise"
    bottom: "conv_g1"
    bottom: "conv_g3"
    top: "sum1"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g4"
    type: "Convolution"
    bottom: "sum1"
    top: "conv_g4"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g4"
    type: "BatchNorm"
    bottom: "conv_g4"
    top: "conv_g4"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g4"
    type: "ReLU"
    bottom: "conv_g4"
    top: "conv_g4"
}
layer {
    name: "conv_g5"
    type: "Convolution"
    bottom: "conv_g4"
    top: "conv_g5"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g5"
    type: "BatchNorm"
    bottom: "conv_g5"
    top: "conv_g5"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum2"
    type: "Eltwise"
    bottom: "sum1"
    bottom: "conv_g5"
    top: "sum2"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g6"
    type: "Convolution"
    bottom: "sum2"
    top: "conv_g6"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g6"
    type: "BatchNorm"
    bottom: "conv_g6"
    top: "conv_g6"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g6"
    type: "ReLU"
    bottom: "conv_g6"
    top: "conv_g6"
}
layer {
    name: "conv_g7"
    type: "Convolution"
    bottom: "conv_g6"
    top: "conv_g7"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g7"
    type: "BatchNorm"
    bottom: "conv_g7"
    top: "conv_g7"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum3"
    type: "Eltwise"
    bottom: "sum2"
    bottom: "conv_g7"
    top: "sum3"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g8"
    type: "Convolution"
    bottom: "sum3"
    top: "conv_g8"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g8"
    type: "BatchNorm"
    bottom: "conv_g8"
    top: "conv_g8"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g8"
    type: "ReLU"
    bottom: "conv_g8"
    top: "conv_g8"
}
layer {
    name: "conv_g9"
    type: "Convolution"
    bottom: "conv_g8"
    top: "conv_g9"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g9"
    type: "BatchNorm"
    bottom: "conv_g9"
    top: "conv_g9"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum4"
    type: "Eltwise"
    bottom: "sum3"
    bottom: "conv_g9"
    top: "sum4"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g10"
    type: "Convolution"
    bottom: "sum4"
    top: "conv_g10"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g10"
    type: "BatchNorm"
    bottom: "conv_g10"
    top: "conv_g10"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g10"
    type: "ReLU"
    bottom: "conv_g10"
    top: "conv_g10"
}
layer {
    name: "conv_g11"
    type: "Convolution"
    bottom: "conv_g10"
    top: "conv_g11"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g11"
    type: "BatchNorm"
    bottom: "conv_g11"
    top: "conv_g11"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum5"
    type: "Eltwise"
    bottom: "sum4"
    bottom: "conv_g11"
    top: "sum5"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g12"
    type: "Convolution"
    bottom: "sum5"
    top: "conv_g12"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g12"
    type: "BatchNorm"
    bottom: "conv_g12"
    top: "conv_g12"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g12"
    type: "ReLU"
    bottom: "conv_g12"
    top: "conv_g12"
}
layer {
    name: "conv_g13"
    type: "Convolution"
    bottom: "conv_g12"
    top: "conv_g13"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g13"
    type: "BatchNorm"
    bottom: "conv_g13"
    top: "conv_g13"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum6"
    type: "Eltwise"
    bottom: "sum5"
    bottom: "conv_g13"
    top: "sum6"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g14"
    type: "Convolution"
    bottom: "sum6"
    top: "conv_g14"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g14"
    type: "BatchNorm"
    bottom: "conv_g14"
    top: "conv_g14"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g14"
    type: "ReLU"
    bottom: "conv_g14"
    top: "conv_g14"
}
layer {
    name: "conv_g15"
    type: "Convolution"
    bottom: "conv_g14"
    top: "conv_g15"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g15"
    type: "BatchNorm"
    bottom: "conv_g15"
    top: "conv_g15"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum7"
    type: "Eltwise"
    bottom: "sum6"
    bottom: "conv_g15"
    top: "sum7"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g16"
    type: "Convolution"
    bottom: "sum7"
    top: "conv_g16"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g16"
    type: "BatchNorm"
    bottom: "conv_g16"
    top: "conv_g16"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g16"
    type: "ReLU"
    bottom: "conv_g16"
    top: "conv_g16"
}
layer {
    name: "conv_g17"
    type: "Convolution"
    bottom: "conv_g16"
    top: "conv_g17"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g17"
    type: "BatchNorm"
    bottom: "conv_g17"
    top: "conv_g17"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum8"
    type: "Eltwise"
    bottom: "sum7"
    bottom: "conv_g17"
    top: "sum8"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g18"
    type: "Convolution"
    bottom: "sum8"
    top: "conv_g18"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g18"
    type: "BatchNorm"
    bottom: "conv_g18"
    top: "conv_g18"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g18"
    type: "ReLU"
    bottom: "conv_g18"
    top: "conv_g18"
}
layer {
    name: "conv_g19"
    type: "Convolution"
    bottom: "conv_g18"
    top: "conv_g19"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g19"
    type: "BatchNorm"
    bottom: "conv_g19"
    top: "conv_g19"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum9"
    type: "Eltwise"
    bottom: "sum8"
    bottom: "conv_g19"
    top: "sum9"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g20"
    type: "Convolution"
    bottom: "sum9"
    top: "conv_g20"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g20"
    type: "BatchNorm"
    bottom: "conv_g20"
    top: "conv_g20"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g20"
    type: "ReLU"
    bottom: "conv_g20"
    top: "conv_g20"
}
layer {
    name: "conv_g21"
    type: "Convolution"
    bottom: "conv_g20"
    top: "conv_g21"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g21"
    type: "BatchNorm"
    bottom: "conv_g21"
    top: "conv_g21"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum10"
    type: "Eltwise"
    bottom: "sum9"
    bottom: "conv_g21"
    top: "sum10"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g22"
    type: "Convolution"
    bottom: "sum10"
    top: "conv_g22"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g22"
    type: "BatchNorm"
    bottom: "conv_g22"
    top: "conv_g22"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g22"
    type: "ReLU"
    bottom: "conv_g22"
    top: "conv_g22"
}
layer {
    name: "conv_g23"
    type: "Convolution"
    bottom: "conv_g22"
    top: "conv_g23"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g23"
    type: "BatchNorm"
    bottom: "conv_g23"
    top: "conv_g23"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum11"
    type: "Eltwise"
    bottom: "sum10"
    bottom: "conv_g23"
    top: "sum11"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}layer {
    name: "conv_g24"
    type: "Convolution"
    bottom: "sum11"
    top: "conv_g24"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g24"
    type: "BatchNorm"
    bottom: "conv_g24"
    top: "conv_g24"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g24"
    type: "ReLU"
    bottom: "conv_g24"
    top: "conv_g24"
}
layer {
    name: "conv_g25"
    type: "Convolution"
    bottom: "conv_g24"
    top: "conv_g25"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g25"
    type: "BatchNorm"
    bottom: "conv_g25"
    top: "conv_g25"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum12"
    type: "Eltwise"
    bottom: "sum11"
    bottom: "conv_g25"
    top: "sum12"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g26"
    type: "Convolution"
    bottom: "sum12"
    top: "conv_g26"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g26"
    type: "BatchNorm"
    bottom: "conv_g26"
    top: "conv_g26"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g26"
    type: "ReLU"
    bottom: "conv_g26"
    top: "conv_g26"
}
layer {
    name: "conv_g27"
    type: "Convolution"
    bottom: "conv_g26"
    top: "conv_g27"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g27"
    type: "BatchNorm"
    bottom: "conv_g27"
    top: "conv_g27"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum13"
    type: "Eltwise"
    bottom: "sum12"
    bottom: "conv_g27"
    top: "sum13"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g28"
    type: "Convolution"
    bottom: "sum13"
    top: "conv_g28"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g28"
    type: "BatchNorm"
    bottom: "conv_g28"
    top: "conv_g28"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g28"
    type: "ReLU"
    bottom: "conv_g28"
    top: "conv_g28"
}
layer {
    name: "conv_g29"
    type: "Convolution"
    bottom: "conv_g28"
    top: "conv_g29"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g29"
    type: "BatchNorm"
    bottom: "conv_g29"
    top: "conv_g29"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum14"
    type: "Eltwise"
    bottom: "sum13"
    bottom: "conv_g29"
    top: "sum14"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g30"
    type: "Convolution"
    bottom: "sum14"
    top: "conv_g30"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g30"
    type: "BatchNorm"
    bottom: "conv_g30"
    top: "conv_g30"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g30"
    type: "ReLU"
    bottom: "conv_g30"
    top: "conv_g30"
}
layer {
    name: "conv_g31"
    type: "Convolution"
    bottom: "conv_g30"
    top: "conv_g31"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g31"
    type: "BatchNorm"
    bottom: "conv_g31"
    top: "conv_g31"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum15"
    type: "Eltwise"
    bottom: "sum14"
    bottom: "conv_g31"
    top: "sum15"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g32"
    type: "Convolution"
    bottom: "sum15"
    top: "conv_g32"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g32"
    type: "BatchNorm"
    bottom: "conv_g32"
    top: "conv_g32"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "relu_g32"
    type: "ReLU"
    bottom: "conv_g32"
    top: "conv_g32"
}
layer {
    name: "conv_g33"
    type: "Convolution"
    bottom: "conv_g32"
    top: "conv_g33"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g33"
    type: "BatchNorm"
    bottom: "conv_g33"
    top: "conv_g33"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum16"
    type: "Eltwise"
    bottom: "sum15"
    bottom: "conv_g33"
    top: "sum16"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g34"
    type: "Convolution"
    bottom: "sum16"
    top: "conv_g34"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "batch_norm_g34"
    type: "BatchNorm"
    bottom: "conv_g34"
    top: "conv_g34"
    param {
        lr_mult: 0
        decay_mult: 0
    }
     param {
        lr_mult: 0
        decay_mult: 0
    
    }
     param {
        lr_mult: 0
        decay_mult: 0
    }    
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "sum17"
    type: "Eltwise"
    bottom: "conv_g1"
    bottom: "conv_g34"
    top: "sum17"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: 1
    }
}
layer {
    name: "conv_g35"
    type: "Convolution"
    bottom: "sum17"
    top: "conv_g35"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer{
	name: "psx2_1"
    type: "Reshape"
    bottom: "conv_g35"
    top: "psx2_1"
    reshape_param {
      pixelshuffler: 2
    }
}
layer {
    name: "relu_g35"
    type: "ReLU"
    bottom: "psx2_1"
    top: "psx2_1"
}
layer {
    name: "conv_g36"
    type: "Convolution"
    bottom: "psx2_1"
    top: "conv_g36"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer{
	name: "psx2_2"
    type: "Reshape"
    bottom: "conv_g36"
    top: "psx2_2"
    reshape_param {
      pixelshuffler: 2
    }
}
layer {
    name: "relu_g36"
    type: "ReLU"
    bottom: "psx2_2"
    top: "psx2_2"
}
layer {
    name: "conv_g37"
    type: "Convolution"
    bottom: "psx2_2"
    top: "conv_g37"
    param {
        lr_mult: 1
    }
    param {
        lr_mult: 0.1
    }
    convolution_param {
        num_output: 3
        kernel_size: 3
        stride: 1
        pad: 1
        gen_mode: true
        iter_size: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
  name: "mse_loss"
  type: "EuclideanLoss"
  bottom: "conv_g37"
  bottom: "label"
  top: "mse_loss"
  gan_loss_param{
    gen_mode: true
    iter_size: 1
  }
}

#-----------   gan gate  ------------
layer {
  name: "gan_gate"
  type: "GANGate"
  bottom: "label"
  bottom: "conv_g37"
  top: "dis_input"
  gan_gate_param{
    iter_size: 1
  }
}
#----------- discrimitive -----------
layer {
  name: "dis_conv_d1"
  type: "Convolution"
  bottom: "dis_input"
  top: "dis_conv_d1"
  param {
    name: "dis_w_1"
    lr_mult: 1
  }
  param {
    name: "dis_b_1"
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d1"
  type: "ReLU"
  bottom: "dis_conv_d1"
  top: "dis_conv_d1"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "dis_conv_d2"
  type: "Convolution"
  bottom: "dis_conv_d1"
  top: "dis_conv_d2"
  param {
    name: "dis_w_2"
    lr_mult: 1
  }
  param {
    name: "dis_b_2"
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d2"
  type: "ReLU"
  bottom: "dis_conv_d2"
  top: "dis_conv_d2"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d2"
  type: "BatchNorm"
  bottom: "dis_conv_d2"
  top: "dis_conv_d2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d3"
  type: "Convolution"
  bottom: "dis_conv_d2"
  top: "dis_conv_d3"
  param {
    name: "dis_w_3"
    lr_mult: 1
  }
  param {
    name: "dis_b_3"
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d3"
  type: "ReLU"
  bottom: "dis_conv_d3"
  top: "dis_conv_d3"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d3"
  type: "BatchNorm"
  bottom: "dis_conv_d3"
  top: "dis_conv_d3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d4"
  type: "Convolution"
  bottom: "dis_conv_d3"
  top: "dis_conv_d4"
  param {
    name: "dis_w_4"
    lr_mult: 1
  }
  param {
    name: "dis_b_4"
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d4"
  type: "ReLU"
  bottom: "dis_conv_d4"
  top: "dis_conv_d4"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d4"
  type: "BatchNorm"
  bottom: "dis_conv_d4"
  top: "dis_conv_d4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d5"
  type: "Convolution"
  bottom: "dis_conv_d4"
  top: "dis_conv_d5"
  param {
    name: "dis_w_5"
    lr_mult: 1
  }
  param {
    name: "dis_b_5"
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d5"
  type: "ReLU"
  bottom: "dis_conv_d5"
  top: "dis_conv_d5"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d5"
  type: "BatchNorm"
  bottom: "dis_conv_d5"
  top: "dis_conv_d5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d6"
  type: "Convolution"
  bottom: "dis_conv_d5"
  top: "dis_conv_d6"
  param {
    name: "dis_w_6"
    lr_mult: 1
  }
  param {
    name: "dis_b_6"
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d6"
  type: "ReLU"
  bottom: "dis_conv_d6"
  top: "dis_conv_d6"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d6"
  type: "BatchNorm"
  bottom: "dis_conv_d6"
  top: "dis_conv_d6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d7"
  type: "Convolution"
  bottom: "dis_conv_d6"
  top: "dis_conv_d7"
  param {
    name: "dis_w_7"
    lr_mult: 1
  }
  param {
    name: "dis_b_7"
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d7"
  type: "ReLU"
  bottom: "dis_conv_d7"
  top: "dis_conv_d7"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d7"
  type: "BatchNorm"
  bottom: "dis_conv_d7"
  top: "dis_conv_d7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "dis_conv_d8"
  type: "Convolution"
  bottom: "dis_conv_d7"
  top: "dis_conv_d8"
  param {
    name: "dis_w_8"
    lr_mult: 1
  }
  param {
    name: "dis_b_8"
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 2
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.02
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_d8"
  type: "ReLU"
  bottom: "dis_conv_d8"
  top: "dis_conv_d8"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "batch_norm_d8"
  type: "BatchNorm"
  bottom: "dis_conv_d8"
  top: "dis_conv_d8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "dis_conv_d8"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 1024
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu_ip1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "ip1"
  top: "score"
  param {
    name: "score_w"
    lr_mult: 1
  }
  param {
    name: "score_b"
    lr_mult: 2
  }
  inner_product_param{
    num_output: 1
    dis_mode: true
    iter_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.0002
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "sigmoid"
  type: "Sigmoid"
  bottom: "score"
  top: "score"
}
layer {
  name: "gan_loss"
  type: "GANDGLoss"
  bottom: "score"
  top: "gan_loss"
  gan_loss_param {
    dis_iter: 1
    gen_iter: 1
    iter_size: 1
    dis_lossweight: 1
    gen_lossweight: 0.001
  }
}
